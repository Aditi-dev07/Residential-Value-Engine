# ğŸ  House Price Prediction â€“ Comparative Study of Regression Models

This project implements and evaluates multiple regression algorithms to predict house prices using a unified preprocessing pipeline. The notebook is structured as a **narrative-driven machine learning workflow** that demonstrates model development, evaluation, and selection using quantitative performance metrics.

The goal is to compare linear, tree-based, and ensemble regressors and select the model that delivers the strongest generalization performance.

---

## ğŸ“Œ Models Implemented

The following regression models are trained and evaluated under identical preprocessing and data splits:

* **Simple Linear Regression (SLR)** â€“ Single-feature baseline
* **Multiple Linear Regression (MLR)** â€“ Multivariate linear baseline
* **Decision Tree Regressor** â€“ Non-linear single-model learner
* **Random Forest Regressor** â€“ Ensemble model for variance reduction

---

## ğŸ“Š Evaluation Metrics

All models are evaluated using:

* **RÂ² Score** â€“ Measures explained variance
* **Root Mean Squared Error (RMSE)** â€“ Measures prediction error

### Final Results

| Model                      | RÂ² Score | RMSE     |
| -------------------------- | -------- | -------- |
| Simple Linear Regression   | 0.35     | 0.31     |
| Multiple Linear Regression | 0.77     | 0.18     |
| Decision Tree Regressor    | 0.55     | 0.26     |
| Random Forest Regressor    | **0.82** | **0.16** |

**Key Gains:**

* Gained a **~134% relative increase in explained variance** from baseline (SLR â†’ Random Forest).
* Gained a **~48% reduction in RMSE** from baseline to final model.
* Gained improved generalization by transitioning from single-model learners to ensemble methods.

---
## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ Predict_house_prices.ipynb     # Main notebook with full ML workflow
â”œâ”€â”€ regression_data_using_scikit_learn.ipynb (optional reference)
â””â”€â”€ README.md
```

---

## â–¶ï¸ How to Run

1. Open the notebook in **Google Colab** or Jupyter Notebook
2. Run all cells sequentially
3. Review:

   * Data preprocessing
   * Model training
   * Metric comparison
   * Final model selection

---

## ğŸ Final Model Selection

The **Random Forest Regressor** is selected as the final model due to:

* Highest explained variance (**RÂ² = 0.82**)
* Lowest prediction error (**RMSE = 0.16**)
* Strong biasâ€“variance tradeoff compared to linear and single-tree models

---

## ğŸš€ Skills Demonstrated

* End-to-end regression pipeline design
* Feature preprocessing & scaling
* Model benchmarking under identical conditions
* Biasâ€“variance tradeoff analysis
* Quantitative performance reporting
* Recruiter-friendly ML documentation

---
